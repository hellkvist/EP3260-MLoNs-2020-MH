{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##imports from libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "\n",
    "# the \"resource\" library is not available on windows. \n",
    "# if it can be imported, we can use it! For example, while running on google colab\n",
    "try:\n",
    "    import resource\n",
    "    print(\"Succesfully imported 'resource' package\")\n",
    "except:\n",
    "    print(\"Failed on importing 'resource' package\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data:\n",
    "We first load the data into ghgdata. We will only use 2000 data points in this implementation, so its enough to use 7 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_sites = 7\n",
    "N_cols = 327\n",
    "dghg = 15\n",
    "Nghg = N_sites * N_cols\n",
    "ghgdata = np.zeros((dghg+1, Nghg))\n",
    "\n",
    "pathprefix = \"./Data/ghg_data/ghg_data/ghg.gid.site\"\n",
    "for i in range(N_sites):\n",
    "    filename = pathprefix + str(i + 1).zfill(4) + \".dat\"\n",
    "    subdata = np.genfromtxt(filename, delimiter=\" \")\n",
    "    ghgdata[:,i*N_cols:(i+1)*N_cols] = subdata\n",
    "\n",
    "ghgdata = ghgdata.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to find the smoothness $L$ of the cost function, given a dataset:\n",
    "We have that for $L$ to guarantee L-smoothness of $f(w)$, it must fulfill the following inequality:\n",
    "$$ L \\succcurlyeq \\frac{1}{4N} \\sum_i y_i^2 x_i x_i^T + 2\\lambda  $$\n",
    "$$  = \\frac{1}{4N} Z^T Z +2\\lambda, $$\n",
    "$$ Z = \\begin{bmatrix}y_1 x_1^T \\\\ \\vdots\\\\ y_N x_N^T\\end{bmatrix}.$$\n",
    "\n",
    "Which is equivalent to $L \\geq \\frac{1}{4N}\\lambda_{\\max}(Z^T Z) + 2\\lambda $, where $\\lambda_{\\max}(Z^T Z)$ is the largest eigenvalue of $Z^T Z$.\n",
    "\n",
    "We implement the finding of the smallest $L$ in the function ``find_smoothness()``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_smoothness(X, Y, lambda_):\n",
    "    # finds smallest L to guarantee L-smoothness of the Logistic Ridge Regression, given data X, Y\n",
    "    # we have found it analytically as the largest eigenvalue of (1/(4N)) * Z.T @ Z + 2 * lambda.\n",
    "    \n",
    "    N, d = X.shape\n",
    "    assert Y.shape == (N,1)\n",
    "    \n",
    "    YX = Y * X\n",
    "    Z = YX.T @ YX\n",
    "    eigs, _ = np.linalg.eig((1/(4*N))*Z)\n",
    "    L = np.max(eigs) + 2*lambda_\n",
    "    \n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of the response variable\n",
    "We use the first 15 variables of the data as inputs $x_i$, and the 16th row as output.\n",
    "We found that using the raw output data came with some numerical issues, due to the products $y_i x_i$ being very large quite often.\n",
    "We therefore threshold the output data into a binary variable in $\\{-1,+1\\}$.\n",
    "\n",
    "We here load the ``ghgdata`` into ``X`` and ``Y`` matrices, and perform the thresholding on the ``Y`` matrix.\n",
    "\n",
    "The plots show histograms of ``Y`` before and after thresholding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ghgdata[:, 0:dghg]\n",
    "Y = ghgdata[:, [dghg]]\n",
    "plt.figure(1)\n",
    "_ = plt.hist(Y, bins=100)\n",
    "\n",
    "# threshold Y to binary\n",
    "threshold = 30\n",
    "mask = Y < threshold\n",
    "Y[mask] = -1\n",
    "Y[~mask] = 1\n",
    "plt.figure(2)\n",
    "_ = plt.hist(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and testing datasets\n",
    "We are not really interested in finding a good classifier of the data, but only to verify our implementation of GD, SGD, SVRG and SAG.\n",
    "Therefore we limit the simulations to only 1000 datapoints for training, to decrease the run time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test data here: (X_train, Y_train, X_test, Y_test)\n",
    "\n",
    "N_train = math.floor(Nghg*0.75)\n",
    "N_test = math.floor(Nghg*0.25) + 1\n",
    "\n",
    "N_train = 1000\n",
    "N_test = 1000\n",
    "\n",
    "# train_idx = np.random.choice(Nghg, size=N_train, replace=False)\n",
    "train_idx = np.arange(N_train)\n",
    "X_train = X[train_idx, :]\n",
    "Y_train = Y[train_idx]\n",
    "\n",
    "# test_idx_bool = ~np.isin(np.arange(Nghg), train_idx)\n",
    "test_idx = np.arange(N_train, N_train + N_test)\n",
    "X_test = X[test_idx, :]\n",
    "Y_test = Y[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Cost and Gradient functions\n",
    "#### The logistic ridge regression\n",
    "$$ f(x) = \\frac{1}{N}\\sum_{i\\in[N]} f_i(x) + \\lambda\\lVert w \\rVert_2^2 $$\n",
    "has the gradient: \n",
    "$$ \\nabla f(x) = \\frac{1}{N}\\sum_{i\\in[N]} \\nabla f_i(x) + 2\\lambda w, $$\n",
    "$$ \\nabla f_i(x) = -y_i x_i \\frac{e^{-y_ix_i^T w}}{1 + e^{-y_ix_i^T w}} = -y_i x_i \\frac{1}{1 + e^{y_i x_i^T w}}.$$\n",
    "We use the latter form for computing gradients, since it has better numerical stability (the exponent can take \"inf\" values, thus the former form is undefined, while the latter is identically zero).\n",
    "\n",
    "We define the gradient estimate for data point $i$ including regularization as:\n",
    "$$  \\hat{g}_i(w) = \\nabla f_i(w) + 2\\lambda w.$$\n",
    "\n",
    "The ``cost()`` function is an implementation of $f(w)$.\n",
    "The ``function_gradient()`` is an implementation of $\\nabla f(w)$, given ``X, Y`` and ``w``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(x, y, w, lambda_ = 0.01):\n",
    "    N, d = x.shape\n",
    "    value = np.sum(np.log(1 + np.exp(- y*x @ w)))\n",
    "    norm_w = np.linalg.norm(w)\n",
    "    c = lambda_ * norm_w ** 2\n",
    "    return value/N + c \n",
    "\n",
    "def function_gradient(X, Y, w, lambda_):\n",
    "    N, d = X.shape\n",
    "    assert Y.shape == (N,1)\n",
    "    assert w.shape == (d,1)\n",
    "    output = np.zeros((N,1))\n",
    "    YX = Y * X # (N,d)\n",
    "    YXw = YX @ w # (N,1)\n",
    "    exp_vec = 1/(1 + np.exp(YXw)) # (N,1)\n",
    "    grad_array = -YX * exp_vec # (N,d)\n",
    "    output = np.sum(grad_array, axis=0) # (1,d)\n",
    "    output = (1/N) * output.reshape(d,1) + 2 * lambda_ * w # (d,1)\n",
    "    return output # (d,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the gradient methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solver(x, y, w, alpha, num_iters, lambda_, epsilon, optimizer = \"GD\", mem=False, return_cost=False, verbose=True):\n",
    "    if (optimizer == \"GD\"):\n",
    "        cost_ = np.zeros(num_iters)\n",
    "        for i in range(num_iters):\n",
    "            g = function_gradient(x, y, w, lambda_) # compute the exact gradient wrt w, given x and y\n",
    "            w = w - alpha * g # GD update\n",
    "            \n",
    "            if return_cost:\n",
    "                cost_[i] = cost(X_test, Y_test, w, lambda_)\n",
    "            \n",
    "            if (i%10==0) and (mem) and ('resource' in sys.modules) and (verbose):\n",
    "                usage=resource.getrusage(resource.RUSAGE_SELF)\n",
    "                print(\"mem for GD (mb):\", (usage[2]*resource.getpagesize())/1000000.0)\n",
    "                        \n",
    "            if (np.linalg.norm(g) <= epsilon) and (verbose):\n",
    "                print(\"GD: Stop on condition fulfilled! Number of iterations: \", i+1, \"/\", num_iters)\n",
    "                break\n",
    "        if verbose:\n",
    "            print(\"GD: Number of iterations: \", num_iters, \"/\", num_iters)\n",
    "            print(\"GD: final norm: \", np.linalg.norm(g))\n",
    "                \n",
    "    elif (optimizer == \"SGD\"):\n",
    "        cost_ = np.zeros(num_iters)\n",
    "        N, d = x.shape\n",
    "        assert y.shape == (N,1)\n",
    "        for i in range(num_iters):\n",
    "            i_ = int(N*np.random.rand(1))\n",
    "            x_ = x[[i_], :]\n",
    "            y_ = y[[i_], :]\n",
    "            \n",
    "            g = function_gradient(x_, y_, w, lambda_) # compute gradient\n",
    "            w = w - alpha * g # GD update \n",
    "            \n",
    "            if return_cost:\n",
    "                cost_[i] = cost(X_test, Y_test, w, lambda_)\n",
    "            \n",
    "            if (i%10==0) and (mem) and ('resource' in sys.modules) and (verbose):\n",
    "                usage=resource.getrusage(resource.RUSAGE_SELF)\n",
    "                print(\"mem for SGD (mb):\", (usage[2]*resource.getpagesize())/1000000.0)\n",
    "                \n",
    "    elif (optimizer == \"SVRG\"):\n",
    "        N, d = x.shape\n",
    "        assert y.shape == (N,1)\n",
    "        \n",
    "        T = 100 # epoch length\n",
    "        K = math.floor(num_iters/T) # number of epochs, given number of iterations\n",
    "        \n",
    "        if K == 0:\n",
    "            # only run for one epoch consisting of num_iters iterations\n",
    "            K = 1\n",
    "            T = num_iters\n",
    "        \n",
    "        cost_ = np.zeros(T*K)\n",
    "        \n",
    "        for k in range(K):\n",
    "            yx = y * x # (N, d)\n",
    "            yxw = yx @ w # (N, 1)\n",
    "            \n",
    "            assert yx.shape == (N, d)\n",
    "            assert yxw.shape == (N, 1)\n",
    "            \n",
    "            # G_ is equal to using function_gradient, but we need G_vec, so we compute it manually here\n",
    "            exp_vec = 1/(1 + np.exp(yxw)) # (N, 1)\n",
    "            \n",
    "            G_vec = (-yx * exp_vec).reshape(d, N) + 2*lambda_*w # (d, N)\n",
    "                                    \n",
    "            G_ = (1/N) * np.sum(G_vec, axis=1, keepdims=1) # (d, 1)\n",
    "            \n",
    "            if (np.linalg.norm(G_) <= epsilon) and (verbose): # check the average gradient\n",
    "                print(\"SVRG: Stop on condition fulfilled! Number of iterations: \", k*t, \"/\", num_iters)\n",
    "                break\n",
    "            \n",
    "            assert G_vec.shape == (d, N)\n",
    "            assert G_.shape == (d, 1)\n",
    "            \n",
    "            for t in range(T):\n",
    "                i_ = int(N*np.random.rand(1))\n",
    "                x_ = x[[i_], :] # (1, d)\n",
    "                y_ = y[[i_], :] # (1, 1)\n",
    "                g = function_gradient(x_, y_, w, lambda_) # (d, 1)\n",
    "                assert g.shape == (d,1)\n",
    "                assert G_vec[:, [i_]].shape == (d,1)\n",
    "                assert G_.shape == (d,1)\n",
    "                w = w - alpha*(g - G_vec[:, [i_]] + G_)\n",
    "                \n",
    "                if return_cost:\n",
    "                    cost_[t + k*T] = cost(X_test, Y_test, w, lambda_)\n",
    "                    \n",
    "                if (t%10==0) and (mem) and ('resource' in sys.modules) and (verbose):\n",
    "                    usage=resource.getrusage(resource.RUSAGE_SELF)\n",
    "                    print(\"mem for SVRG (mb):\", (usage[2]*resource.getpagesize())/1000000.0)\n",
    "                    \n",
    "        if (K > 0) and (verbose):\n",
    "            print(\"SVRG: final norm: \", np.linalg.norm(g))\n",
    "            \n",
    "    elif (optimizer == \"SAG\"):\n",
    "        N, d = x.shape\n",
    "        assert y.shape == (N,1)\n",
    "        \n",
    "        # compute initial gradient estimates for all datapoints\n",
    "        yx = y * x # (N, d)\n",
    "        yxw = yx @ w # (N, 1)\n",
    "        exp_vec = 1/(1 + np.exp(yxw)) # (N, 1)\n",
    "#         G_vec = (-yx * exp_vec).reshape(d, N) + 2 * lambda_ * w # (d, N)\n",
    "        G_vec = np.zeros((d, N))\n",
    "        cost_ = np.zeros(num_iters)\n",
    "        for k in range(num_iters):\n",
    "            #set_trace()\n",
    "            i_ = int(N*np.random.rand(1))\n",
    "            x_ = x[[i_], :] # (1, d)\n",
    "            y_ = y[[i_], :] # (1, 1)\n",
    "            g = function_gradient(x_, y_, w, lambda_) # (d, 1)\n",
    "            G_vec[:, [i_]] = g # (d, N)\n",
    "            w = w - (alpha/N) * np.sum(G_vec, axis=1, keepdims=1) # (d, 1)\n",
    "            if return_cost:\n",
    "                cost_[k] = cost(X_test, Y_test, w, lambda_)\n",
    "                \n",
    "            if (np.linalg.norm(g) <= epsilon) and (verbose):\n",
    "                    print(\"Stop on condition fulfilled! Number of iterations: \", k, \"/\", num_iters)\n",
    "                    break\n",
    "                    \n",
    "            if (t%10==0) and (mem) and ('resource' in sys.modules) and (verbose):\n",
    "                    usage=resource.getrusage(resource.RUSAGE_SELF)\n",
    "                    print(\"mem for SAG (mb):\", (usage[2]*resource.getpagesize())/1000000.0)\n",
    "        if verbose:\n",
    "            print(\"final norm: \", np.linalg.norm(g))\n",
    "    if return_cost:\n",
    "        return w, cost_\n",
    "    else:\n",
    "        return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the constant stepsize\n",
    "In these implementations we are using constant stepsize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set regularization $\\lambda$ and gradient stop condition $\\varepsilon$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define solvers: GD, SGD, SVRG and SAG. \n",
    "# Setting the values here:\n",
    "\n",
    "lambda_ = 10 # change the value 1e-6 is good for full matrices\n",
    "epsilon = 0.000001 # change the value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load ``Y_train`` and ``X_train`` into ``y`` and ``x``.\n",
    "Initial guess ``w``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Y_train\n",
    "x = X_train\n",
    "print(\"x: \", x.shape)\n",
    "print(\"y: \", y.shape)\n",
    "N, d = x.shape\n",
    "w = np.random.rand(d,1)*0.1  # Initialization of w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smoothness and convexity\n",
    "L = find_smoothness(x, y, lambda_)\n",
    "print(\"L: \", L)\n",
    "scaling = 2\n",
    "alpha = 1/(scaling*L)\n",
    "alpha_string = \"alpha = 1/({0}*L)\".format(scaling) + \": \"\n",
    "print(alpha_string, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------- GD Solver -----------------------\n",
    "print(alpha)\n",
    "num_iters = 10000 # change the value\n",
    "start = time.time()\n",
    "gd = solver(x, y, w, alpha, num_iters, lambda_, epsilon, optimizer = \"GD\", mem=False)\n",
    "end = time.time()\n",
    "print(\"Weights of GD after convergence: \\n\", gd.flatten())\n",
    "cost_value = cost(X_test, Y_test, gd, lambda_) \n",
    "print(\"Cost of GD after convergence: \", cost_value)\n",
    "\n",
    "print(\"Training time for GD: \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------- SGD Solver -----------------------\n",
    "num_iters = 10 # change the value\n",
    "start = time.time()\n",
    "sgd = solver(x, y, w, alpha, num_iters, lambda_, epsilon, optimizer = \"SGD\", mem=False)\n",
    "end = time.time()\n",
    "print(\"Weights of SGD after convergence: \\n\", sgd.flatten())\n",
    "\n",
    "cost_value = cost(X_test, Y_test, sgd, lambda_)  # Calculate the cost value\n",
    "print(\"Cost of SGD after convergence: \", cost_value)\n",
    "\n",
    "print(\"Training time for SGD: \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------- SVRG Solver -----------------------\n",
    "num_iters = 10000 # change the value\n",
    "start = time.time()\n",
    "svrg = solver(x, y, w, alpha, num_iters, lambda_, epsilon, optimizer=\"SVRG\", mem=False)\n",
    "end = time.time()\n",
    "print(\"\\nWeights of SVRG after convergence: \\n\", svrg.flatten())\n",
    "\n",
    "cost_value = cost(X_test, Y_test, svrg, lambda_)\n",
    "print(\"Cost of SVRG after convergence: \", cost_value)\n",
    "print(\"Training time for SVRG: \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------- SAG Solver -----------------------\n",
    "num_iters = 10000 # change the value\n",
    "start = time.time()\n",
    "sag = solver(x, y, w, alpha, num_iters, lambda_, epsilon, optimizer=\"SAG\", mem=False)\n",
    "end = time.time()\n",
    "print(\"Weights of SAG after convergence: \\n\", sag.flatten())\n",
    "\n",
    "cost_value = cost(X_test, Y_test, sag, lambda_)\n",
    "print(\"Cost of SAG after convergence: \", cost_value)\n",
    "print(\"Training time for SAG: \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Executing the iterations and plot the cost function here:\n",
    "I_max = 100\n",
    "ti= np.zeros((I_max,4))\n",
    "cost_= np.zeros((I_max,4))\n",
    "w = np.random.rand(d,1)*0.01\n",
    "for i in range(I_max):\n",
    "    print(\"......\",i,\".......\")\n",
    "    #--------------GD-------------------\n",
    "    start = time.time()\n",
    "    gd = solver(x, y, w, alpha, i+1, lambda_, epsilon, optimizer=\"GD\", mem=False)\n",
    "    end = time.time()\n",
    "\n",
    "    cost_[i,0] = cost(X_test, Y_test, gd, lambda_)\n",
    "\n",
    "    ti[i,0] = end-start\n",
    "\n",
    "    #---------------SGD------------------\n",
    "    start = time.time()\n",
    "    sgd = solver(x, y, w, alpha, i+1, lambda_, epsilon, optimizer=\"SGD\", mem=False)\n",
    "    end = time.time()\n",
    "\n",
    "    cost_[i,1] = cost(X_test, Y_test, sgd, lambda_)\n",
    "\n",
    "    ti[i,1] = end-start\n",
    "    \n",
    "    #---------------SVRG----------------\n",
    "    start = time.time()\n",
    "    svrg = solver(x, y, w, alpha, i+1, lambda_, epsilon, optimizer=\"SVRG\", mem=False)\n",
    "    end = time.time()\n",
    "\n",
    "    cost_[i,2] = cost(X_test, Y_test, svrg, lambda_)\n",
    "\n",
    "    ti[i,2] = end-start\n",
    "    \n",
    "    #---------------SAG------------------\n",
    "    start = time.time()\n",
    "    sag = solver(x, y, w, alpha, i+1, lambda_, epsilon, optimizer=\"SAG\", mem=False)\n",
    "    end = time.time()\n",
    "\n",
    "    cost_[i,3] = cost(X_test, Y_test, sag, lambda_)\n",
    "\n",
    "    ti[i,3] = end-start\n",
    "    \n",
    "    #------------------------------------\n",
    "    \n",
    "    ## Pl the results:\n",
    "    \n",
    "\n",
    "l0 = plt.plot(cost_[:,0],color=\"C3\")\n",
    "l1 = plt.plot(cost_[:,1],color=\"C2\")\n",
    "l2 = plt.plot(cost_[:,2],color=\"C1\")\n",
    "l3 = plt.plot(cost_[:,3],color=\"C0\")\n",
    "# complete other plots here: \n",
    "\n",
    "\n",
    "plt.xlabel(\"Number of Iteration\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.legend(['GD', 'SGD', 'SVRG', 'SAG'])\n",
    "plt.show()\n",
    "\n",
    "l0 = plt.plot(ti[:,0],color=\"C3\")\n",
    "l1 = plt.plot(ti[:,1],color=\"C2\")\n",
    "l2 = plt.plot(ti[:,2],color=\"C1\")\n",
    "l3 = plt.plot(ti[:,3],color=\"C0\")\n",
    "# complete other plots here:\n",
    "\n",
    "plt.xlabel(\"Number of Iteration\")\n",
    "plt.ylabel(\"Time (sec)\")\n",
    "plt.legend(['GD', 'SGD', 'SVRG', 'SAG'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Executing the iterations and plot the cost function here:\n",
    "print(alpha_string, alpha)\n",
    "n_sims = 10\n",
    "I_max = 10000\n",
    "ti= np.zeros((I_max,4))\n",
    "cost_= np.zeros((I_max,4))\n",
    "for n in range(n_sims):\n",
    "    w = np.random.rand(d,1)*0.01\n",
    "    \n",
    "    print(\"......\",n+1,\".......\")\n",
    "    #--------------GD-------------------\n",
    "    gd, gd_cost  = solver(x, y, w, alpha, I_max, lambda_, epsilon, optimizer=\"GD\", mem=False, return_cost=True, verbose=False)\n",
    "\n",
    "    cost_[:,0] += gd_cost/n_sims\n",
    "\n",
    "    #---------------SGD------------------\n",
    "    sgd, sgd_cost = solver(x, y, w, alpha, I_max, lambda_, epsilon, optimizer=\"SGD\", mem=False, return_cost=True, verbose=False)\n",
    "\n",
    "    cost_[:,1] += sgd_cost/n_sims\n",
    "    \n",
    "    #---------------SVRG----------------\n",
    "    svrg, svrg_cost = solver(x, y, w, alpha, I_max, lambda_, epsilon, optimizer=\"SVRG\", mem=False, return_cost=True, verbose=False)\n",
    "\n",
    "    cost_[:,2] += svrg_cost/n_sims\n",
    "    \n",
    "    #---------------SAG------------------\n",
    "    sag, sag_cost = solver(x, y, w, alpha, I_max, lambda_, epsilon, optimizer=\"SAG\", mem=False, return_cost=True, verbose=False)\n",
    "    \n",
    "    cost_[:,3] += sag_cost/n_sims\n",
    "    \n",
    "    #------------------------------------\n",
    "## PLOTs\n",
    "l0 = plt.plot(cost_[:,0],color=\"C3\")\n",
    "l1 = plt.plot(cost_[:,1],color=\"C2\")\n",
    "l2 = plt.plot(cost_[:,2],color=\"C1\")\n",
    "l3 = plt.plot(cost_[:,3],color=\"C0\")\n",
    "# complete other plots here: \n",
    "\n",
    "plt.xlabel(\"Number of Iteration\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.legend(['GD', 'SGD', 'SVRG', 'SAG'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOTs\n",
    "l0 = plt.plot(cost_[:,0],color=\"C3\")\n",
    "l1 = plt.plot(cost_[:,1],color=\"C2\")\n",
    "l2 = plt.plot(cost_[:,2],color=\"C1\")\n",
    "l3 = plt.plot(cost_[:,3],color=\"C0\")\n",
    "# complete other plots here: \n",
    "\n",
    "plt.xlabel(\"Number of Iteration\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.legend(['GD', 'SGD', 'SVRG', 'SAG'])\n",
    "\n",
    "plt.ylim(0.25, .35)\n",
    "plt.xlim(15000,20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tunning the hyper-paramter here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Comparing different optimizers here: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
